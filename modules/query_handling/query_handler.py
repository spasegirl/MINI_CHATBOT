from modules.gpt_module import llm
import logging

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


def rag_pipeline(user_input, retriever=None, image_description= None):
    """
    Unified pipeline for both RetrievalQA and direct LLM interaction.
    
    Args:
        user_input (str): The query or input from the user.
        retriever (Retriever, optional): The retriever object for document search. Defaults to None.
    
    Returns:
        str: Response generated by the LLM.
    """
    context = ""
    
    # retrieval only if a retriever is provided
    if image_description:
        context = f"Image description: {image_description}"
    # Otherwise, try to retrieve documents if a retriever is provided
    elif retriever:
        try:
            retrieved_docs = retriever.get_relevant_documents(user_input)
            if retrieved_docs:
                # Combine retrieved document chunks into a single context
                context = "\n".join(doc.page_content for doc in retrieved_docs)
        except Exception as e:
            logger.error(f"Error during document retrieval: {e}")
    
    # Fallback if no context is available
    if not context:
        context = "No relevant context found."
    

    prompt_text = f"""
    You are an assistant that can answer questions based on a document or provide general assistance.
    When the context is provided, answer using the context; otherwise, respond directly to the user's query.

    Context: {context}
    Question: {user_input}
    Answer:
    """
    
    # response from the LLM
    response = llm.invoke([{"role": "user", "content": prompt_text}])
    
    return response.content.strip()

