import logging
from modules.gpt_module import llm


# Needs adjustment to work with the new structure
# Setting up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def execute_query(retrieval_chain, user_query):
    """
    Executes a query using the provided retrieval chain.

    Args:
        retrieval_chain (Runnable): The retrieval chain to process the query.
        user_query (str): The user's query.

    Returns:
        str: The response generated by the chain.
    """
    # Ensure the user_query is a string (it could be a dictionary, fix it here)
    if isinstance(user_query, dict):
        # If it's a dictionary, extract the question
        user_query = user_query.get("question", "")

    if not isinstance(user_query, str):
        raise ValueError("The user query must be a string.")

    try:
        # Now pass the query properly to the retrieval chain
        response = retrieval_chain.invoke({"question": user_query})
        return response
    except Exception as e:
        return str(e)


def chat_with_gpt(prompt):
    """Get a response from GPT-3 given a prompt."""
    try:
        # Directly send the prompt to GPT model
        response = llm.invoke([{"role": "user", "content": prompt}])
        return response.content.strip()
    except Exception as e:
        return str(e)
