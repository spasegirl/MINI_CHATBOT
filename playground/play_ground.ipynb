{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=api_endpoint,\n",
    "    azure_deployment=\"text-embedding-ada-002\", \n",
    "    api_version=\"2024-05-01-preview\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import AzureOpenAIEmbeddings, OpenAI, AzureChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain_community.llms import AzureOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import tiktoken\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import numpy as np\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    model_name=\"gpt-35-turbo-0613\",\n",
    "    azure_endpoint=api_endpoint,\n",
    "    openai_api_key=api_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    request_timeout=60\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"static\\uploads\\CertificateOfCompletion_5.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "re_char_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "chunks = re_char_splitter.split_documents(documents)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_search = FAISS.from_documents(chunks, embedding_model)\n",
    "chain = RetrievalQA.from_chain_type(llm, chain_type='stuff', retriever=doc_search.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\U765123\\AppData\\Local\\Temp\\ipykernel_12928\\410705963.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain.run(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document is a certificate of completion for Mohadeseh Yousofy who successfully completed the course \"Information Security Awareness Refresher\" on June 5th, 2024.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the docuemtn about?\"\n",
    "\n",
    "result = chain.run(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an AI language model, I do not have emotions or feelings. But I am functioning well and ready to assist you with any task or question you may have. How can I assist you today?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt= \"Hoes it going?\"\n",
    "\n",
    "response = llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "response.content.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document-related query...\n",
      "Document Answer: The document is a certificate of completion for a course on Information Security Awareness Refresher taken by Mohadeseh Yousofy on June 5, 2024.\n",
      "Processing general conversational query...\n",
      "As an AI language model, I don't have emotions. But I'm functioning properly and ready to assist you in any way I can. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "def handle_user_input(user_input):\n",
    "    # Determine intent (simple keyword matching here; can replace with ML-based intent classifier)\n",
    "    if \"document\" in user_input.lower() or \"pdf\" in user_input.lower():\n",
    "        print(\"Processing document-related query...\")\n",
    "        try:\n",
    "            result = chain.run(user_input)\n",
    "            return f\"Document Answer: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error processing document query: {e}\"\n",
    "    else:\n",
    "        print(\"Processing general conversational query...\")\n",
    "        try:\n",
    "            response = llm.invoke([{\"role\": \"user\", \"content\": user_input}])\n",
    "            return response.content.strip()\n",
    "        except Exception as e:\n",
    "            return f\"Error processing conversational query: {e}\"\n",
    "\n",
    "\n",
    "user_input_1 = \"What is the document about?\"\n",
    "response_1 = handle_user_input(user_input_1)\n",
    "print(response_1)\n",
    "\n",
    "user_input_2 = \"How's it going?\"\n",
    "response_2 = handle_user_input(user_input_2)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = doc_search.as_retriever()\n",
    "\n",
    "template = \"\"\"You are an assistant that can answer questions based on a document or provide general assistance.\n",
    "When the context is provided, answer using the context; otherwise, respond directly to the user's query.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def rag_pipeline(user_input):\n",
    "    retrieved_docs = retriever.get_relevant_documents(user_input)\n",
    "    if retrieved_docs:\n",
    "\n",
    "        context = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    else:\n",
    "        context = \"No relevant context found.\"  \n",
    "    \n",
    "    full_prompt = prompt.format(context=context, question=user_input)\n",
    "    \n",
    "\n",
    "    response = llm.invoke([{\"role\": \"user\", \"content\": full_prompt}])\n",
    "    \n",
    "    return response.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\U765123\\AppData\\Local\\Temp\\ipykernel_12928\\1457494831.py:14: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(user_input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document is a Certificate of Completion for Mohadeseh Yousofy who successfully completed the course on Information Security Awareness Refresher on 5 June 2024.\n",
      "As an AI language model, I do not have feelings or emotions. However, I am ready and available to assist you with any questions or tasks you may have. Is there anything specific you need help with?\n"
     ]
    }
   ],
   "source": [
    "user_input_1 = \"What is the document about?\"\n",
    "response_1 = rag_pipeline(user_input_1)\n",
    "print(response_1)\n",
    "\n",
    "user_input_2 = \"How's it going?\"\n",
    "response_2 = rag_pipeline(user_input_2)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document is a Certificate of Completion for the course \"Information Security Awareness Refresher\" completed by Mohadeseh Yousofy on 5 June 2024.\n",
      "As an AI language model, I do not have emotions, but I am here to assist you with any questions or concerns you may have. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "retriever = doc_search.as_retriever()\n",
    "def rag_pipeline(user_input, retriever=None):\n",
    "    \"\"\"\n",
    "    Unified pipeline for both RetrievalQA and direct LLM interaction.\n",
    "    \n",
    "    Args:\n",
    "        user_input (str): The query or input from the user.\n",
    "        retriever (Retriever, optional): The retriever object for document search. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        str: Response generated by the LLM.\n",
    "    \"\"\"\n",
    "    context = \"\"\n",
    "    \n",
    "    # retrieval only if a retriever is provided\n",
    "    if retriever:\n",
    "        retrieved_docs = retriever.get_relevant_documents(user_input)\n",
    "        \n",
    "        if retrieved_docs:\n",
    "            # Combining retrieved document chunks into a single context\n",
    "            context = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    \n",
    "    if not context:\n",
    "        context = \"No relevant context found.\"  \n",
    "\n",
    "    prompt_text = f\"\"\"\n",
    "    You are an assistant that can answer questions based on a document or provide general assistance.\n",
    "    When the context is provided, answer using the context; otherwise, respond directly to the user's query.\n",
    "\n",
    "    Context: {context}\n",
    "    Question: {user_input}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([{\"role\": \"user\", \"content\": prompt_text}])\n",
    "    \n",
    "    return response.content.strip()\n",
    "\n",
    "\n",
    "\n",
    "user_input_1 = \"What is the document about?\"\n",
    "response_1 = rag_pipeline(user_input_1, retriever=retriever) \n",
    "print(response_1)\n",
    "\n",
    "user_input_2 = \"How's it going?\"\n",
    "response_2 = rag_pipeline(user_input_2)  \n",
    "print(response_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
